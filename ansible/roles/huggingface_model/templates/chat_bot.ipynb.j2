{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c096071e-0d33-4598-bd8f-3f616db77daa",
   "metadata": {},
   "source": [
    "## Import all modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a318d-8b9b-41b3-8284-3fb223ef5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7fd44b7-b3d5-4ad5-8bf6-3ca9ea630e9e",
   "metadata": {},
   "source": [
    "## üß† Check GPU Availability\n",
    "\n",
    "This checks whether:\n",
    "- PyTorch can detect a CUDA-enabled GPU\n",
    "- The GPU name is reported correctly\n",
    "\n",
    "If `CUDA available` is `False`, the model will run on CPU, which is **much slower**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c176b1a1-218d-4f5b-b74b-f7e3b37843c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"‚ö† Warning: No GPU detected ‚Äî inference will be VERY slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cbf0c-1773-4f38-9d0a-9b56826c35d2",
   "metadata": {},
   "source": [
    "## üìÅ Choose Model Location\n",
    "\n",
    "We specify the local directory where our model files were downloaded.\n",
    "This prevents needing to fetch from the Hugging Face Hub each time.\n",
    "\n",
    "You can update this path if your model is saved somewhere else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caccd42a-d999-45ca-a5e3-56bea0974aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from: /opt/llm/models/Mistral-7B-Instruct/\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/opt/llm/models/Mistral-7B-Instruct/\"\n",
    "print(f\"Using model from: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a4df-6c6f-4712-a1c0-3f301fe472d4",
   "metadata": {},
   "source": [
    "## üöÄ Load Tokenizer and Model Into Memory\n",
    "\n",
    "Here‚Äôs what happens in this cell:\n",
    "\n",
    "1. Load the tokenizer ‚Äî converts text ‚Üî tokens\n",
    "2. Ensure the tokenizer has a valid padding token (avoids warnings)\n",
    "3. Load the model weights from disk\n",
    "4. Place the model automatically on GPU if available\n",
    "\n",
    "This is the **slowest** step and uses GPU VRAM, so we see a progress bar.\n",
    "Once complete, the model is ready to generate text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf97d81-f6e7-4924-b38b-4d1cb9e8a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925eca70330e45daa438613f63736603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded into memory!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,   \n",
    "    device_map=\"auto\",          \n",
    ")\n",
    "\n",
    "print(\"Model successfully loaded into memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402a1d3-1488-4046-8f3b-ca4a0ac21191",
   "metadata": {},
   "source": [
    "## üí¨ Format the Prompt for an Instruction Model\n",
    "\n",
    "Mistral models use a special instruction format:\n",
    "\n",
    "- This tells the model exactly where the user question starts and ends.\n",
    "\n",
    "- We print the formatted prompt so we can visually confirm it is structured correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb3870b-6abc-42d6-b376-825f82759da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Prompt:\n",
      "<s>[INST] How does a CPU work? [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How does a CPU work?\"\n",
    "\n",
    "formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "print(\"\\nüìù Prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb3008-8e98-46b0-a3a2-e973ce4431f6",
   "metadata": {},
   "source": [
    "## üî§ Tokenize the Prompt and Move to GPU\n",
    "\n",
    "We convert text into numeric tokens.\n",
    "If a GPU is available, we move those tensors to the GPU memory.\n",
    "\n",
    "This ensures the model can run efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e17887-2fb7-4b48-a8b6-32f67a68620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt has been tokenised!\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "print('Prompt has been tokenised!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdb5f3-c80d-46cf-82e2-b7ec46742077",
   "metadata": {},
   "source": [
    "## ü§ñ Generate and Display the Model's Reply\n",
    "\n",
    "Steps happening here:\n",
    "\n",
    "1. Call `model.generate()` to produce new tokens\n",
    "2. Convert output token IDs back into text\n",
    "3. Extract only the model‚Äôs answer (skip the prompt)\n",
    "4. Print the final assistant response\n",
    "\n",
    "Success here means the model is fully functional!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6a81fa-39a6-4eb6-9dad-47c3ebb68ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output...\n",
      "\n",
      "ü§ñ Model Response:\n",
      "A CPU, or Central Processing Unit, is the brain of a computer. It's responsible for carrying out all the operations and instructions of a computer program. At the most basic level, a CPU performs calculations and logical operations on data.\n",
      "\n",
      "A CPU is made up of two main parts: the control unit and the arithmetic logic unit (ALU). The control unit fetches instructions from memory and decodes them, while the ALU performs the actual operations on the data.\n",
      "\n",
      "When a program is executed, the CPU first fetches the instructions from memory and decodes them to determine what operation needs to be performed. It then fetches the necessary data from memory or registers, performs the operation on the data, and\n"
     ]
    }
   ],
   "source": [
    "# 9Ô∏è‚É£ Generate output tokens\n",
    "print('Generating output...')\n",
    "output_tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# üîü Decode tokens ‚Üí readable text\n",
    "decoded_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the assistant reply after [/INST]\n",
    "if \"[/INST]\" in decoded_text:\n",
    "    decoded_text = decoded_text.split(\"[/INST]\", 1)[1].strip()\n",
    "\n",
    "print(\"\\nü§ñ Model Response:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca23b1-06e4-4798-9ef8-52cfdeb9e805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
